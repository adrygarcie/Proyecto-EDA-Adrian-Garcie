{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "#from .pagination import scrape_data_for_days  # Importamos la función de paginación\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Ejecutar en segundo plano\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_data_from_page(url, current_date):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Lógica de extracción de datos (como lo tenías antes)\n",
    "    # Aquí obtienes los datos de la tabla y creas un DataFrame\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    table = soup.find(\"table\", class_=\"table-scroll\")\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 10:\n",
    "                data.append([\n",
    "                    cols[0].text.strip(),\n",
    "                    cols[1].text.strip(),\n",
    "                    cols[2].text.strip(),\n",
    "                    cols[3].text.strip(),\n",
    "                    cols[5].text.strip(),\n",
    "                    cols[6].text.strip(),\n",
    "                    cols[7].text.strip(),\n",
    "                    cols[9].text.strip(),\n",
    "                    cols[10].text.strip()\n",
    "                ])\n",
    "        columns = [\"Date & Time UTC\", \"Lat. degrees\", \"Lon. degrees\", \"Depth km\",\n",
    "                   \"Region\", \"Type\", \"A/M\", \"Magnitude\", \"Network\"]\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        return df\n",
    "    return pd.DataFrame()  # Si no encuentra la tabla, devuelve un DataFrame vacío.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from .scrape_page import extract_data_from_page  # Importamos la función para extraer datos\n",
    "\n",
    "def scrape_data_for_days(start_date, num_days, driver):\n",
    "    all_data = pd.DataFrame()\n",
    "    current_date = start_date\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        print(f\"Scrapeando datos para la fecha: {current_date}\")\n",
    "\n",
    "        # Crear la URL con la fecha\n",
    "        url = f\"https://www.emsc-csem.org/Earthquake_data/index.php?date={current_date}\"\n",
    "\n",
    "        # Extraer los datos\n",
    "        df = extract_data_from_page(url, current_date)\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "        # Intentar encontrar el botón \"Next\" para pasar de página\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"//div[@class='pag spes spes1']\"))\n",
    "            )\n",
    "            if next_button.is_displayed():\n",
    "                next_button.click()\n",
    "                print(\"Clic en el botón 'Next'\")\n",
    "                time.sleep(1)  # Esperar para que se cargue la siguiente página\n",
    "            else:\n",
    "                print(\"No hay más páginas.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(\"Error al encontrar el botón 'Next':\", e)\n",
    "            break\n",
    "\n",
    "        # Restar un día para el próximo ciclo\n",
    "        current_date = (pd.to_datetime(current_date) - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el driver de Selenium\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def main():\n",
    "    # Configuración de Selenium\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Ejecutar en segundo plano\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Fecha de inicio y número de días\n",
    "    start_date = '2025-04-01'  # Fecha de inicio\n",
    "    num_days = 5  # Número de días a scrapear\n",
    "\n",
    "    # Llamar a la función para scrapear los datos\n",
    "    scraped_data = scrape_data_for_days(start_date, num_days, driver)\n",
    "\n",
    "    # Obtener la ruta absoluta del directorio donde está este script\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "    # Construir la ruta relativa a la carpeta 'data'\n",
    "    data_dir = os.path.join(current_dir, \"..\", \"data\")\n",
    "    os.makedirs(data_dir, exist_ok=True)  # Crear la carpeta si no existe\n",
    "\n",
    "    # Ruta completa del archivo CSV\n",
    "    csv_path = os.path.join(data_dir, \"scraped_earthquakes.csv\")\n",
    "    \n",
    "    # Guardar el DataFrame en CSV\n",
    "    scraped_data.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Datos guardados en: {csv_path}\")\n",
    "\n",
    "    driver.quit()  # Cerrar el navegador al final\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapeando datos para la fecha: 2025-04-01\n",
      "Clic en el botón 'Next'\n",
      "Scrapeando datos para la fecha: 2025-03-31\n",
      "Clic en el botón 'Next'\n",
      "Scrapeando datos para la fecha: 2025-03-30\n",
      "Clic en el botón 'Next'\n",
      "Scrapeando datos para la fecha: 2025-03-29\n",
      "Clic en el botón 'Next'\n",
      "Scrapeando datos para la fecha: 2025-03-28\n",
      "Clic en el botón 'Next'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Call the main function\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m scraped_data \u001b[38;5;241m=\u001b[39m scrape_data_for_days(start_date, num_days, driver)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Obtener la ruta absoluta del directorio donde está este script\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Construir la ruta relativa a la carpeta 'data'\u001b[39;00m\n\u001b[0;32m     25\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_proyecto_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
